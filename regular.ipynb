{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow.keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "from IPython import display as ipythondisplay\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import glob\n",
    "import cv2\n",
    "print(len(tf.config.list_physical_devices('GPU')) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLoader(object):\n",
    "    def __init__(self, data_dir):\n",
    "        print(\"Currently Opening\")\n",
    "        sys.stdout.flush()\n",
    "        self.cache = h5py.File(data_dir, 'r')\n",
    "        print(\"Loading\")\n",
    "        sys.stdout.flush()\n",
    "        self.imgs = self.cache['images'][:]\n",
    "        self.labels = self.cache['labels'][:].astype(np.float32)\n",
    "        self.img_dim = self.imgs.shape\n",
    "        sample_num = self.img_dim[0]\n",
    "        self.train_idxs = np.random.permutation(np.arange(sample_num))\n",
    "        self.positive_train_idxs = self.train_idxs[self.labels[self.train_idxs, 0] == 1.0]\n",
    "        self.negative_train_idxs = self.train_idxs[self.labels[self.train_idxs, 0] != 1.0]\n",
    "\n",
    "    def size(self):\n",
    "        return self.train_idxs.shape[0]\n",
    "    \n",
    "    def step_per_epoch(self, batch_size):\n",
    "        return self.size()//10//batch_size\n",
    "    \n",
    "    def gen_batch(self, n, p_positive=None, p_negative=None):\n",
    "        positive_idxs = np.random.choice(self.positive_train_idxs, size=n//2, replace=False, p=p_positive)\n",
    "        negative_idxs = np.random.choice(self.negative_train_idxs, size=n//2, replace=False, p=p_negative)\n",
    "        idxs = np.concatenate((positive_idxs, negative_idxs))\n",
    "        sorted_idxs = np.sort(idxs)\n",
    "        img = (self.imgs[sorted_idxs,:,:,::-1]/255.).astype(np.float32)\n",
    "        label = self.labels[sorted_idxs,...]\n",
    "        return (img, label)\n",
    "    \n",
    "    def all_faces(self):\n",
    "        return self.imgs[self.positive_train_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering data from an online source. Found it while doing the deep learning specialization with stanford and deeplearning.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Opening\n",
      "Loading\n"
     ]
    }
   ],
   "source": [
    "training_path = tensorflow.keras.utils.get_file('train_face.h5', 'https://www.dropbox.com/s/hlz8atheyozp1yx/train_face.h5?dl=1')\n",
    "loader = TrainLoader(training_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a regular classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_creation(outputs=1):\n",
    "    Conv = functools.partial(tensorflow.keras.layers.Conv2D, padding='same', activation='relu')\n",
    "    BatchNorm = tensorflow.keras.layers.BatchNormalization\n",
    "    Flatten = tensorflow.keras.layers.Flatten\n",
    "    Dense = functools.partial(tensorflow.keras.layers.Dense, activation='relu')\n",
    "    model = tensorflow.keras.Sequential([\n",
    "        Conv(filters=1*12, kernel_size=5,  strides=2),\n",
    "        BatchNorm(),\n",
    "        Conv(filters=2*12, kernel_size=5,  strides=2),\n",
    "        BatchNorm(),\n",
    "        Conv(filters=4*12, kernel_size=3,  strides=2),\n",
    "        BatchNorm(),\n",
    "        Conv(filters=6*12, kernel_size=3,  strides=2),\n",
    "        BatchNorm(),\n",
    "        Flatten(),\n",
    "        Dense(512),\n",
    "        Dense(outputs, activation=None),\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_classifier = classifier_creation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "lr = 1e-4\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3434/3434 [00:26<00:00, 131.27it/s]\n",
      "100%|██████████| 3434/3434 [00:23<00:00, 143.56it/s]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def train_step(imgs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = regular_classifier(imgs)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "    #defining backprop\n",
    "    gradients = tape.gradient(loss, regular_classifier.trainable_variables)\n",
    "    opt.apply_gradients(zip(gradients, regular_classifier.trainable_variables))\n",
    "    return loss\n",
    "def training_loop():\n",
    "    for epoch in range(epochs):\n",
    "        for idx in tqdm(range(loader.size()//batch_size)):\n",
    "            imgs, labels = loader.gen_batch(batch_size)\n",
    "            loss = train_step(imgs, labels)\n",
    "\n",
    "training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the Accuracy of the normal classifier on the bias dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regular Classifier Accuracy: 0.9970\n"
     ]
    }
   ],
   "source": [
    "(b_x, b_y) = loader.gen_batch(1000)\n",
    "y_pred = tf.round(tf.nn.sigmoid(regular_classifier.predict(b_x)))\n",
    "reg_acc = tf.reduce_mean(tf.cast(tf.equal(b_y, y_pred), tf.float32))\n",
    "\n",
    "print(\"Regular Classifier Accuracy: {:.4f}\".format(reg_acc.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ethics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a040d8fe03181393046e76fdc216320101f74c2eba5995ceefdaa224029eff2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
